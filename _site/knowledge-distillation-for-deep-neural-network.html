<!doctype html>
<html>

<head>

  <title>
    
      Knowledge Distilation for Deep Neural Network | Machine Learning
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Machine Learning" />
  <!-- RSS-v2.0
  <link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Machine Learning | Document my learning notes."/>
  //-->


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Knowledge Distilation for Deep Neural Network | Machine Learning</title>
<meta name="generator" content="Jekyll v3.6.3" />
<meta property="og:title" content="Knowledge Distilation for Deep Neural Network" />
<meta name="author" content="Hoang Pham" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep Neural Networks (DNN) have recently achieved great success in many natural language processing as well as computer vision tasks. However, they are becoming increasingly deeper, complex, computationally expensive and memory intensive. Some representatives of this trend are the deep language representation models, which includes BERT, ELMo, and GPT. This hinders their deployment in devices with low resources or in applications with strict latency requirements. Therefor, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance." />
<meta property="og:description" content="Deep Neural Networks (DNN) have recently achieved great success in many natural language processing as well as computer vision tasks. However, they are becoming increasingly deeper, complex, computationally expensive and memory intensive. Some representatives of this trend are the deep language representation models, which includes BERT, ELMo, and GPT. This hinders their deployment in devices with low resources or in applications with strict latency requirements. Therefor, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance." />
<link rel="canonical" href="http://localhost:4000/knowledge-distillation-for-deep-neural-network.html" />
<meta property="og:url" content="http://localhost:4000/knowledge-distillation-for-deep-neural-network.html" />
<meta property="og:site_name" content="Machine Learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-09-04T00:00:00+07:00" />
<script type="application/ld+json">
{"datePublished":"2019-09-04T00:00:00+07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/knowledge-distillation-for-deep-neural-network.html"},"@type":"BlogPosting","url":"http://localhost:4000/knowledge-distillation-for-deep-neural-network.html","author":{"@type":"Person","name":"Hoang Pham"},"headline":"Knowledge Distilation for Deep Neural Network","description":"Deep Neural Networks (DNN) have recently achieved great success in many natural language processing as well as computer vision tasks. However, they are becoming increasingly deeper, complex, computationally expensive and memory intensive. Some representatives of this trend are the deep language representation models, which includes BERT, ELMo, and GPT. This hinders their deployment in devices with low resources or in applications with strict latency requirements. Therefor, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance.","dateModified":"2019-09-04T00:00:00+07:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

<div class="container">
  <header class="site-header">
  <h3 class="site-title">
    <a href="/">Machine Learning</a>
  </h3>
  <nav class="menu-list">
    
      <a href="/pages/about.html" class="menu-link">About</a>
    
      <a href="/pages/contact.html" class="menu-link">Contact</a>
    

    
      <a href="https://www.facebook.com/darkrai.angel" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
    
      <a href="https://www.linkedin.com/in/hoang-pham-4b6590164/" class="menu-link" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
    
      <a href="https://github.com/hugo-cinner" class="menu-link" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
    
      <a href="mailto:hoang.a3.lqd@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    
  </nav>
  <div class="dropdown">
    <button class="dropbtn"><i class="fa fa-bars" aria-hidden="true"></i></button>
    <div class="dropdown-content">
      
        <a href="/pages/about.html" class="menu-link">About</a>
      
        <a href="/pages/contact.html" class="menu-link">Contact</a>
      

      
        <a href="https://www.facebook.com/darkrai.angel" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      
        <a href="https://www.linkedin.com/in/hoang-pham-4b6590164/" class="menu-link" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
      
        <a href="https://github.com/hugo-cinner" class="menu-link" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
      
        <a href="mailto:hoang.a3.lqd@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
      
    </div>
  </div>
</header>

  <div class="posts-wrapper">
    <div class="page-content">
  <h1>
    Knowledge Distilation for Deep Neural Network
  </h1>

  <span class="post-date">
    Written on
    
    September
    4th,
    2019
    by
    
      Hoang Pham
    
  </span>

  

  <article>
    <blockquote>
  <p>Deep Neural Networks (DNN) have recently achieved great success in many natural language processing as well as computer vision tasks. However, they are becoming increasingly deeper, complex, computationally expensive and memory intensive. Some representatives of this trend are the deep language representation models, which includes BERT, ELMo, and GPT. This hinders their deployment in devices with low resources or in applications with strict latency requirements. Therefor, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance.</p>
</blockquote>

<!--more-->

<ul class="table-of-content" id="markdown-toc">
  <li><a href="#model-compression-and-acceleration" id="markdown-toc-model-compression-and-acceleration">Model Compression and Acceleration</a>    <ul>
      <li><a href="#what-is-model-compression-and-acceleration" id="markdown-toc-what-is-model-compression-and-acceleration">What is Model Compression and Acceleration?</a></li>
      <li><a href="#compacting-and-accelerating-nns-model-techniques" id="markdown-toc-compacting-and-accelerating-nns-model-techniques">Compacting and Accelerating NNs model techniques</a></li>
    </ul>
  </li>
  <li><a href="#knowledge-distillation" id="markdown-toc-knowledge-distillation">Knowledge Distillation</a></li>
  <li><a href="#summarization" id="markdown-toc-summarization">Summarization</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<hr />

<h2 id="model-compression-and-acceleration">Model Compression and Acceleration</h2>
<blockquote>
  <p>In this part, I will introduce you model compression/acceleration and its four schemes: parameter pruning and sharing, low-rank factorization, transferred / compact convolutional filters, and knowledge distillation.</p>
</blockquote>

<h3 id="what-is-model-compression-and-acceleration">What is Model Compression and Acceleration?</h3>

<h3 id="compacting-and-accelerating-nns-model-techniques">Compacting and Accelerating NNs model techniques</h3>
<p><img src="assets/images/knowledge_distillation/summarize these four types of methods.png" alt="Summarize 4 types of methods" />
In Table I, we briefly summarize these four types of methods [1]. Generally, the parameter pruning &amp; sharing, low-rank factorization and knowledge distillation approaches can be used in DNN models with fully connected layers and convolutional layers, achieving comparable performances. While methods using transferred/compact filters are designed for models with convolutional layers only. In addition, regarding the training protocols, models based on parameter pruning/sharing low-rank factorization can be extracted from pre-trained ones or trained from scratch. While the transferred/compact filter and knowledge distillation models can only support train from scratch.</p>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>In this research, we will focus on Knowledge Distillation method which can compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. The main idea of KD based approaches is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax.</p>

<p>In [2], knowledge distillation is described as a compression framework which eased the training of deep networks by following a student-teacher paradigm, in which the student was penalized according to a softened version of the teacher’s output. The student was trained to predict the output and the classification labels. Despite its simplicity, it demonstrates promising results in various image classification tasks. It is validated on MNIST dataset and experimental results show that these methods match or outperform the teacher’s performance, while requiring notably fewer parameters and multiplications. Moreover, this research also investigate the effects of ensembling Deep Neural Network (DNN) acoustic models that are used in Automatic Speech Recognition (ASR). It shows that the distillation strategy that they propose in this paper achieves the desired effect of distilling an ensemble of models into a single model that works significantly better than a model of the same size that is learned directly from the same training data.</p>

<p><img src="assets/images/knowledge_distillation/frame_classification_acc_and_WER.png" alt="Frame classification acc and WER" /></p>

<p>The work in [3] propose to distill knowledge from BERT, a state-of- the-art language representation model, into a single-layer BiLSTM and achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time across multiple datasets in paraphrasing, natural language inference, and sentiment classification.</p>

<p><img src="assets/images/knowledge_distillation/BiLSTM_test_result.png" alt="BiLSTM test result" /></p>

<p>With this approach, a shallow BiLSTM based model achieves results comparable to Embeddings from Language Models (ELMo; Peters et al., 2018), but uses around 100 times fewer parameters and performs inference 15 times faster. 
The methods of knowledge distillation provide many benefits such as directly accelerating model without special hardware or implementations. It is still worthy developing KD-based approaches and exploring how to improve their performances.</p>

<h2 id="summarization">Summarization</h2>

<p>I believe that if we know how different parts of a DNN behave, we can design student networks more cleverly and let them learn the teacher’s most important internal representations. Through that it help to transform our complex model to a simple model which could be run on CPU or deployed at the edge devices. Beside, in the scope of work of our company, I strongly believe that Knowledge Distillation can help to speed up our models as well as save the training costs. For example, some ML models of our company are using BERT in training and inference, if we can re-implement the work of [3] to create a small version of BERT, it will help a lot in reducing their storage and computational cost.</p>

<h2 id="reference">Reference</h2>

<p>[1] <a href="https://arxiv.org/abs/1710.09282">Yu Cheng, Duo Wang, Pan Zhou, Member, IEEE, and Tao Zhang, Senior Member, IEEE, “A Survey of Model Compression and Acceleration for Deep Neural Networks”.</a></p>

<p>[2] <a href="https://arxiv.org/abs/1503.02531">G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network” CoRR, vol. Abs/1503.02531, 2015.</a></p>

<p>[3] <a href="https://arxiv.org/abs/1903.12136">Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy Lin, “Distilling Task-Specific Knowledge from BERT into Simple Neural Networks”.</a></p>

<hr />

<p><em>If you notice mistakes and errors in this post, don’t hesitate to contact me at [hoang dot a3 dot lqd at gmail dot com] and I would be super happy to correct them right away!</em></p>


  </article>

<!--  <div class="post-share">-->
<!--    <div class="post-date">Feel free to share!</div>-->
<!--    <div class="sharing-icons">-->
<!--      <a href="https://twitter.com/intent/tweet?text=Knowledge Distilation for Deep Neural Network&amp;url=/knowledge-distillation-for-deep-neural-network.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>-->
<!--      <a href="https://www.facebook.com/sharer/sharer.php?u=/knowledge-distillation-for-deep-neural-network.html&amp;title=Knowledge Distilation for Deep Neural Network" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>-->
<!--      <a href="https://plus.google.com/share?url=/knowledge-distillation-for-deep-neural-network.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>-->
<!--    </div>-->
<!--  </div>-->

  <div class="related">
    <h2>You may also enjoy...</h2>
    
    <ul class="related-posts">
      
        
          
          
        
          
            <li>
              <h3>
                <a href="/an-overview-of-machine-learning.html">
                  <div class="related-thumbnail">
                    
                  </div>
                  <div class="related-title">
                    An Overview of Machine Learning
                  </div>
                  <!--<small>August 13, 2019</small>-->
                </a>
              </h3>
            </li>
            
          
        
      
        
          
          
        
          
          
        
      
    </ul>
  </div>

  
    <section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = "millennial-3";
    var disqus_identifier = "/knowledge-distillation-for-deep-neural-network.html";
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

  

</div>

  </div>
  <footer class="footer">
  
    <a href="https://www.facebook.com/darkrai.angel" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
  
    <a href="https://www.linkedin.com/in/hoang-pham-4b6590164/" class="menu-link" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  
    <a href="https://github.com/hugo-cinner" class="menu-link" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  
    <a href="mailto:hoang.a3.lqd@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  
  <div class="post-date"><a href="/">Machine Learning | Document my learning notes. by Hoang Pham</a></div>
</footer>

</div>

</body>
</html>
